"""
üöÄ IntelliLog-AI ‚Äî Production Model Training Pipeline (v4.0)

Ensemble model training with:
- XGBoost
- LightGBM  
- Ensemble stacking
- Cross-validation
- Model versioning
- Performance tracking

Author: Vivek Marri
Version: 4.0.0 (Production-Ready)
"""

import os
import json
import joblib
import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import xgboost as xgb
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("‚ö†Ô∏è  LightGBM not available. Install with: pip install lightgbm")

import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelTrainer:
    """Production-grade model training with ensemble and versioning."""
    
    def __init__(self, model_dir: str = "models"):
        self.model_dir = model_dir
        os.makedirs(model_dir, exist_ok=True)
        self.models = {}
        self.metrics = {}
        
    def train_xgboost(self, X_train, y_train, X_val, y_val):
        """Train XGBoost model with optimized hyperparameters."""
        logger.info("Training XGBoost model...")
        
        params = {
            'objective': 'reg:squarederror',
            'max_depth': 8,
            'learning_rate': 0.05,
            'n_estimators': 500,
            'min_child_weight': 3,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'gamma': 0.1,
            'reg_alpha': 0.1,
            'reg_lambda': 1.0,
            'random_state': 42,
            'n_jobs': -1
        }
        
        model = xgb.XGBRegressor(**params)
        
        # Train with early stopping
        model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            verbose=False
        )
        
        # Predictions
        train_pred = model.predict(X_train)
        val_pred = model.predict(X_val)
        
        # Metrics
        metrics = {
            'train_mae': mean_absolute_error(y_train, train_pred),
            'train_rmse': np.sqrt(mean_squared_error(y_train, train_pred)),
            'train_r2': r2_score(y_train, train_pred),
            'val_mae': mean_absolute_error(y_val, val_pred),
            'val_rmse': np.sqrt(mean_squared_error(y_val, val_pred)),
            'val_r2': r2_score(y_val, val_pred),
        }
        
        logger.info(f"‚úì XGBoost - Val MAE: {metrics['val_mae']:.3f}, Val R¬≤: {metrics['val_r2']:.4f}")
        
        return model, metrics
    
    def train_lightgbm(self, X_train, y_train, X_val, y_val):
        """Train LightGBM model with optimized hyperparameters."""
        if not LIGHTGBM_AVAILABLE:
            logger.warning("LightGBM not available, skipping...")
            return None, None
            
        logger.info("Training LightGBM model...")
        
        params = {
            'objective': 'regression',
            'metric': 'mae',
            'max_depth': 8,
            'learning_rate': 0.05,
            'n_estimators': 500,
            'num_leaves': 50,
            'min_child_samples': 20,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'reg_alpha': 0.1,
            'reg_lambda': 1.0,
            'random_state': 42,
            'n_jobs': -1,
            'verbose': -1
        }
        
        model = lgb.LGBMRegressor(**params)
        
        # Train with early stopping
        model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
        )
        
        # Predictions
        train_pred = model.predict(X_train)
        val_pred = model.predict(X_val)
        
        # Metrics
        metrics = {
            'train_mae': mean_absolute_error(y_train, train_pred),
            'train_rmse': np.sqrt(mean_squared_error(y_train, train_pred)),
            'train_r2': r2_score(y_train, train_pred),
            'val_mae': mean_absolute_error(y_val, val_pred),
            'val_rmse': np.sqrt(mean_squared_error(y_val, val_pred)),
            'val_r2': r2_score(y_val, val_pred),
        }
        
        logger.info(f"‚úì LightGBM - Val MAE: {metrics['val_mae']:.3f}, Val R¬≤: {metrics['val_r2']:.4f}")
        
        return model, metrics
    
    def train_ensemble(self, X_train, y_train, X_val, y_val):
        """Train ensemble of models and combine predictions."""
        logger.info("Training ensemble models...")
        
        # Train individual models
        xgb_model, xgb_metrics = self.train_xgboost(X_train, y_train, X_val, y_val)
        lgb_model, lgb_metrics = self.train_lightgbm(X_train, y_train, X_val, y_val)
        
        self.models['xgboost'] = xgb_model
        self.metrics['xgboost'] = xgb_metrics
        
        if lgb_model is not None:
            self.models['lightgbm'] = lgb_model
            self.metrics['lightgbm'] = lgb_metrics
            
            # Create ensemble predictions (weighted average)
            xgb_val_pred = xgb_model.predict(X_val)
            lgb_val_pred = lgb_model.predict(X_val)
            
            # Weight based on validation performance (inverse MAE)
            xgb_weight = 1 / xgb_metrics['val_mae']
            lgb_weight = 1 / lgb_metrics['val_mae']
            total_weight = xgb_weight + lgb_weight
            
            ensemble_pred = (xgb_val_pred * xgb_weight + lgb_val_pred * lgb_weight) / total_weight
            
            ensemble_metrics = {
                'val_mae': mean_absolute_error(y_val, ensemble_pred),
                'val_rmse': np.sqrt(mean_squared_error(y_val, ensemble_pred)),
                'val_r2': r2_score(y_val, ensemble_pred),
                'xgb_weight': xgb_weight / total_weight,
                'lgb_weight': lgb_weight / total_weight
            }
            
            self.metrics['ensemble'] = ensemble_metrics
            logger.info(f"‚úì Ensemble - Val MAE: {ensemble_metrics['val_mae']:.3f}, Val R¬≤: {ensemble_metrics['val_r2']:.4f}")
        else:
            logger.info("Using XGBoost only (LightGBM not available)")
            self.metrics['ensemble'] = xgb_metrics
    
    def save_models(self, version: str = None):
        """Save models with versioning."""
        if version is None:
            version = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        version_dir = os.path.join(self.model_dir, f"v_{version}")
        os.makedirs(version_dir, exist_ok=True)
        
        # Save models
        for name, model in self.models.items():
            if model is not None:
                model_path = os.path.join(version_dir, f"{name}_model.pkl")
                joblib.dump(model, model_path)
                logger.info(f"‚úì Saved {name} model to {model_path}")
        
        # Save metrics
        metrics_path = os.path.join(version_dir, "metrics.json")
        with open(metrics_path, 'w') as f:
            json.dump(self.metrics, f, indent=2)
        logger.info(f"‚úì Saved metrics to {metrics_path}")
        
        # Save best model to main directory (for production use)
        best_model = self.models['xgboost']  # Default to XGBoost
        best_model_path = os.path.join(self.model_dir, "xgb_delivery_time_model.pkl")
        joblib.dump(best_model, best_model_path)
        logger.info(f"‚úì Saved production model to {best_model_path}")
        
        # Save version info
        version_info = {
            'version': version,
            'timestamp': datetime.now().isoformat(),
            'metrics': self.metrics,
            'models': list(self.models.keys())
        }
        
        version_info_path = os.path.join(self.model_dir, "latest_version.json")
        with open(version_info_path, 'w') as f:
            json.dump(version_info, f, indent=2)
        
        return version_dir


def main():
    """Main training pipeline."""
    print("\n" + "="*70)
    print("üöÄ IntelliLog-AI Production Model Training Pipeline")
    print("="*70 + "\n")
    
    # Load processed data
    data_path = 'data/processed/training_data_enhanced.csv'
    
    if not os.path.exists(data_path):
        print(f"‚ùå Training data not found at {data_path}")
        print("   Run: python src/features/build_features_enhanced.py first")
        return
    
    logger.info(f"Loading data from {data_path}...")
    df = pd.read_csv(data_path)
    
    # Separate features and target
    target_col = 'delivery_time_min'
    
    # Feature columns (exclude target and metadata)
    exclude_cols = [target_col, 'order_time', 'distance_category', 'weight_category', 
                    'traffic', 'weather', 'order_type']
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    
    X = df[feature_cols]
    y = df[target_col]
    
    logger.info(f"Dataset: {len(df)} samples, {len(feature_cols)} features")
    logger.info(f"Target statistics: mean={y.mean():.2f}, std={y.std():.2f}, min={y.min():.2f}, max={y.max():.2f}")
    
    # Train/validation split
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    logger.info(f"Train set: {len(X_train)} samples")
    logger.info(f"Validation set: {len(X_val)} samples")
    
    # Train models
    trainer = ModelTrainer()
    trainer.train_ensemble(X_train, y_train, X_val, y_val)
    
    # Save models
    version = datetime.now().strftime("%Y%m%d_%H%M%S")
    version_dir = trainer.save_models(version=version)
    
    # Print summary
    print("\n" + "="*70)
    print("‚úÖ Training Complete!")
    print("="*70)
    print(f"\nVersion: {version}")
    print(f"Saved to: {version_dir}")
    print("\nModel Performance:")
    print("-" * 70)
    
    for model_name, metrics in trainer.metrics.items():
        if 'val_mae' in metrics:
            print(f"\n{model_name.upper()}:")
            print(f"  Validation MAE:  {metrics['val_mae']:.3f} minutes")
            print(f"  Validation RMSE: {metrics['val_rmse']:.3f} minutes")
            print(f"  Validation R¬≤:   {metrics['val_r2']:.4f}")
    
    print("\n" + "="*70)
    print("üéØ Production Model Ready!")
    print("="*70)
    print(f"\nProduction model: models/xgb_delivery_time_model.pkl")
    print(f"Features used: {len(feature_cols)}")
    print("\nNext steps:")
    print("  1. Test model predictions")
    print("  2. Deploy to production")
    print("  3. Monitor performance")
    print("  4. Set up retraining pipeline")


if __name__ == "__main__":
    main()
